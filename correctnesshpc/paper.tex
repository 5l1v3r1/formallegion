\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables

% Dejavu fonts have unicode character support
\usepackage{DejaVuSansMono}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{listings} 
\usepackage{fancyvrb}

\usepackage{amssymb}
\usepackage{bbm}
\usepackage[greek,english]{babel}
\usepackage{textgreek}

%\usepackage{fontspec}
%\defaultfontfeatures{Ligatures=TeX}

\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage{autofe}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\settopmatter{printacmref=false}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[Correctness'17]{Correctness 2017: First International Workshop on
  Software Correctness for HPC Applications}{November 2017}{Denver Colorado, USA} 
\acmYear{2017}
\copyrightyear{2017}

%\acmPrice{15.00}

\begin{document}

\title{Towards Correct Legion}
\author{George Stelle}
\affiliation{\institution{Los Alamos National Laboratory}}
\email{stelleg@lanl.gov}

\author{Noah Evans}
\affiliation{\institution{Sandia National Laboratories}}
\email{nevans@sandia.gov}

\author{Pat McCormick}
\affiliation{\institution{Los Alamos National Laboratory}}
\email{pat@lanl.gov}

\begin{abstract}
Legion is a programming model that uses logical regions of memory to reason about 
locality and concurrency. We formalize the Legion semantics in a mechanized
proof assistant, Coq, and show how this enables one to reason formally and
prove theorems about the semantics. We also discuss how to extend this work to
prove that serial semantics are preserved and how given enough time, one could
extend the effort to prove an implementation is correct.
\end{abstract}

\maketitle

\section{Intro}
High performance computing, as the name implies, has long focused on
performance. Expensive machines are purchased to run increasingly complex
simulations, and every optimization can save huge amounts of money, both due to
the initial cost of the machine and the cost of power to run it. 

That said, what good is a fast computation if we aren't certain that the
values it computes are correct? Presumably, these computations inform important
decisions in areas ranging from climate to nuclear safety. We should strive to
be as confident as possible that we aren't basing these decisions on buggy
code.

The strongest guarantees of correctness come from the area of formal
specifications and proofs. A specification is a description of how a program
should behave, while proofs are used to show that an implementation conforms to
a specification. \emph{Formal} specifications and proofs are defined as
specifications and proofs that have been embedded in a machine-checked formal
logic \cite{bertot2013interactive, nipkow2002isabelle, pfenning1999system, jackson1994nuprl}. This process of formalizing specifications and
proofs in machine checked logics has been shown to catch common errors in
previously informal proofs \cite{clarke1996formal}, and therefore is now considered
to provide the highest assurance of correctness currently possible \cite{?}.

In addition, recent work has shown that modular, composable formal
specifications enable formal correctness proofs for large software systems that
were previously unattainable \cite{weng2016deepspec}. By integrating a formal specification
tightly with an implementation, one can achieve the highest levels of
confidence in code currently possible \cite{weng2016deepspec}. For example, the Compcert
project implements an optimizing C compiler and proves it correct. The success
of Compcert in removing compiler bugs has been well documented. We will discuss
later how Compcert could be used in building verifiable HPC systems.

In HPC, programming models have become an increasingly important way to ensure
that code stays maintainable, performant, and portable \cite{diaz2012survey}. We claim that
these programming models provide a good target for formalization: any
correctness arguments can be leveraged by every program using the model. It is
exactly this kind of modularization of proofs that has been so successful in
proving programs correct \cite{weng2016deepspec}.

For this paper, we take a recent programming model, Legion, and formalize its
semantics and type system. We then prove a few supporting lemmas as examples of
the kinds of things one can prove in an HPC settings. Finally, we discuss the
some of the possible paths towards full, formally correct implementation. This
is no small task, as we discuss later, and the ways forward are numerous and
difficult. But if we believe that HPC informs important decisions, we must
rise to the challenge.

\section{Background}

Legion is a programming model for high performance computing that enables
reasoning about locality and concurrency via logical regions. It has the
attractive property that it is guaranteed to preserve serial semantics for
memory operations within logical regions. This allows programmers to avoid the
burden of reasoning about the non-determinism that arises from concurrent
computation. For this paper, we'll be formalizating work on from Triecher et.
al \cite{treichler2013language}. While there has been much work extending Legion since then,
the core semantics has stayed relatively stable \cite{slaughter2015regent, deppart}. 

By partitioning regions into \emph{subregions}, and tracking \emph{privileges}, 
Legion is able to ensure that certain code won't interfere or alias, and can
therefore be safely, at least with respect to those memory regions, run in
parallel. This theory has been put into practice to impressive effect, and has
been shown to scale both within a node and between \cite{bauer2012legion, treichler2014realm}. That
said, there are still challenges --subtle bugs that occasionally show up in the
implementation. It is possible that these kinds of bugs could manifest
themselves in ways that would lead to incorrect conclusions when informing
major decisions, something we'd generally like to avoid. One goal of this paper
is to move towards formally and systematically removing \emph{all} bugs from
the programming model, giving everyone who uses is far more certainty that 
their code is correct.

To move towards this goal, we turn to mechanized proof assistants.
Improvements to proof assistants over recent decades have made them a powerful
tool for implementing and reasoning about code. From provably correct compilers
to provably correct operating systems, proof assistants enable extremely high
certainty in correctness even for large scale software projects. 

Coq, in particular, has proven itself to be a particularly potent dependently
typed proof assistant. At its core is the Calculus of Inductive Constructions,
a small dependently typed programming language and logic, capable of powerful
abstractions for formal proofs \cite{bertot2013interactive}. In the last 10 years great strides
have been made in using it to both build systems and prove useful properties
about them \cite{leroy2012compcert, anand2017certicoq, chlipala2009effective}. 

Specifications come in a wide range of formats. Many, like the OpenMP
specification, are very informal, primarily consisting of english descriptions
of how different constructs should behave \cite{dagum1998openmp}. Others, like Legion's,
are written by hand in the using standard notation for logical relations, etc.
For this paper, when we refer to a \emph{formal} specification, we are
referring to a specification defined in a formal logic, eg. Coq. 

While semantics defined by hand as logical relations are a huge improvement
over natural language specifications, there is still much room for improvement.
\cite{formalspec vs paper} has shown that even hand-written mathematical
specifications and proofs in computer science papers almost always have errors
that can be caught by a formal proof.  This is not to say, of course, that
formal specifications and proofs are infallible.  On the contrary, it is quite
easy to get a specification wrong. That said, often, if a there is a mistake in
a spec , the process of proving it formally will expose that mistake, allowing
for its correction.  

\section{A Formal Specification of Legion}

A program without a specification cannot be incorrect, it can only be
surprising \footnote{Paraphrase of J.J. Horning, 1982}. One of the most
important and challenging aspects of proving correctness is defining a formal
specification of how a program should behave. Indeed, there is currently a
large NSF effort involving many of the best minds in programming languages
dedicated to improving our ability to do just this \cite{weng2016deepspec}. The choice
of specification has far-reaching consequences in how easy it is to later prove
correctness properties. 

One thing that makes Legion unique among high performance runtime systems is
that it provides a specification of its operational semantics \cite{treichler2013language}.
We take this specification of and formalize it in Coq, enabling us to prove
lemmas and theorems about it. See Figure~\ref{syntax} for the Coq
implementation of the syntax, and Figure~\ref{semantics}and semantics. All code
is available online at \texttt{https://github.com/lanl/formallegion}. 

\begin{figure}
\centering
\begin{BVerbatim}
Inductive e :=
  | bv : bool → e
  | iv : nat → e
  | etuple : list e → e
  | eindex : e → nat → e
  | id : var → e
  | new : T → r → e
  | null : T → r → e
  | isnull : e → e
  | upr : e → list r → e
  | downr : e → list r → e
  | read : e → e 
  | write : e → e → e
  | reduce : var → e → e
  | newcolor : r → e
  | color : e → e → e → e
  | add : e → e → e
  | lt : e → e → e
  | elet : var → T → e → e → e
  | eif : e → e → e → e
  | call : var → list r → list e → e
  | partition : r → e → list r → e → e
  | pack : e → T → list r → e
  | unpack : e → var → T → list r → e → e.
\end{BVerbatim}
\caption{Legion expression syntax}
\label{syntax}
\end{figure}

The Legion semantics are defined as a big step operational semantics, which
result in non-deterministic orderings of memory reads and writes. By defining a
notion of \texttt{valid\_interleaves}, we can reason formally about what evaluations 
can happen concurrently. This should allow us to prove formally that if the
runtime makes scheduling decisions based on region permissions in a particular
way, serial semantics are preserved with respect to the resulting heap state
and return value. This is a desirable property: it means that any code using
Legion can reason, either formally or informally, about its code with respect
to a simple serial semantics, and have those semantics be preserved for
parallel execution. This is in contrast to programming models like Cilk, which
claim a weaker property: a serial execution is one of many valid executions. 

\begin{figure*}
\centering
\begin{BVerbatim}
Reserved Notation " input ↦ output " (at level 50).
Inductive eval: Map r ρ 
              * Map var v
              * Map l T
              * Map l v
              * list l 
              * e 
              → v * list memop
              → Prop := 
  | ERead1 : ∀ M L H S C e l E v,
    (M, L, H, S, C, e) ↦ (vl l, E) →
    l ∉ C → 
    lookup l (apply E S) = Some v → 
    (M, L, H, S, C, read e) ↦ (v, E ++ [mread l v])
  | ERead2 : ∀ M L H S C e l E v,
    (M, L, H, S, C, e) ↦ (vl l, E) →
    l ∈ C → 
    (M, L, H, S, C, read e) ↦ (v, E ++ [mread l v]) 
  | EWrite : ∀ M L H S S' C e1 e2 l E1 E2 E v,
    (M, L, H, S, C, e1) ↦ (vl l, E1) →
    (M, L, H, S', C, e2) ↦ (v, E2) →
    valid_interleave S C E [E1; E2] →
    (M, L, H, S, C, write e1 e2) ↦ (vl l, E ++ [mwrite l v])
  | ENew : ∀ M L H S C t l r, 
    l ∈ lu r M →
    l ∉ domain S →
    (M, L, H, S, C, new t r) ↦ (vl l, [])
  | EUpRgn : ∀ M L H S C e v E rs,
    (M, L, H, S, C, e) ↦ (v, E) →
    (M, L, H, S, C, upr e rs) ↦ (v, E)
  | EColor : ∀ M L H S C K E1 E2 E3 E l e1 e2 e3 v, 
    (M, L, H, S, C, e1) ↦ (vcoloring K, E1) → 
    (M, L, H, apply E1 S, C, e2) ↦ (vl l, E2) → 
    (M, L, H, apply E2 (apply E1 S), C, e3) ↦ (viv v, E3) → 
    valid_interleave S C E [E1; E2; E3] →
    (M, L, H, S, C, color e1 e2 e3) ↦ (vcoloring ((l,v)::K), E) 
  | EPartition : ∀ M M' L H S C rs rp ρs e1 e2 K E' E1 E2 v, 
    (M, L, H, S, C, e1) ↦ (vcoloring K, E1) → 
    ρs = map (map fst) (groupBy (λ x y, beq_nat (snd x) (snd y)) K) →
    M' = zip rs ρs ++ M →  
    (M', L, H, S, C, e2) ↦ (v, E2) →
    valid_interleave S C E' [E1; E2] →
    (M, L, H, S, C, partition rp e1 rs e2) ↦ (v, E')  
  | ECall : ∀ M L H S C es vs Es xs id rs En1 E'' E' M' L' S' C' t ts rs' e Phi Q v, 
    (∀ e v E, In (e, (v, E)) (zip es (zip vs Es)) → (M, L, H, S, C, e) ↦ (v, E)) →
    valid_interleave S C E' Es →
    M' = zip rs' (map (λ r, lu r M) rs) →  
    L' = zip xs vs →
    S' = apply E' S → 
    function id rs xs ts Phi Q t e →
    (M', L', H, S', C', e) ↦ (v, En1) → 
    valid_interleave S C E'' [E'; En1] →
    (M, L, H, S, C, call id rs es) ↦ (v, E'')
    ...
where " input ↦ output " := (eval input output).
\end{BVerbatim}
\caption{Fragment of Legion's formal semantics defined in Coq}
\label{semantics}
\end{figure*}

The language is made up of some standard elements, e.g. \texttt{call},
\texttt{id}, and some Legion-specific constructs e.g. \texttt{partition},
\texttt{upr}. Evaluation is similar in nature, with the standard elements
having standard semantics, and the 

\section{Proofs}

While so far we have discussed the formalization of the \emph{specification},
we have largely left discussion of proofs by the wayside. In this section, we
discuss the challenges of proving useful properties of real world systems, and
give some examples of basic lemmas that will be useful in proving correctness
properties of Legion. The hope is that the reader will gain some insight into
the challenges involved in proving formal properties of systems of real-world
systems.

One of the core notions of Legion is the notion of region relations. These
describe how regions are related, either by being disjoint, or by being
subregions of one another. See Figure~\ref{regionrel} for the syntax. Another
important construction are mappers, \texttt{M}, which map logical regions
\texttt{r} with a set of physical locations. We include some definitions as
well, notably the notion that a mapping respects (\texttt{resp}) a set of
region requirements iff for every subregion relation and disjoint relation, the
mapping to sets of physical locations follows the corresponding set relation.

\begin{figure*}
\centering
\begin{BVerbatim}
Inductive ω := 
  | subr : r → r → ω 
  | disj : r → r → ω.
Definition Ω := set ω.

Reserved Notation " ω ∈* Ω " (at level 40).
Inductive cclo (Ω : Ω) : ω → Prop := 
  | trivial : ∀ ω, ω ∈ Ω → ω ∈* Ω
  | sub_refl : ∀ r, subr r r ∈* Ω
  | sub_trans : ∀ ri rj rk, subr ri rj ∈* Ω → subr rj rk ∈* Ω  → 
                                  subr ri rk ∈* Ω 
  | dis_sub : ∀ ri rj rk, subr ri rj ∈* Ω → disj rj rk ∈*  Ω → 
                                  disj ri rk ∈* Ω
  | dis_comm : ∀ ri rj, disj ri rj ∈* Ω → disj rj ri ∈* Ω 
where " ω ∈* Ω " := (cclo Ω ω).

Definition resp (M : M) (elem : Ω → ω → Prop) (Ω : Ω) :=
  (∀ r1 r2, elem Ω (subr r1 r2) → lu r1 M ⊆ lu r2 M)
                              ∧ 
  (∀ r1 r2, elem Ω (disj r1 r2) → lu r1 M ∩ lu r2 M = [])
.

Notation " M ~ Ω " := (resp M (λ Ω ω, In ω Ω) Ω) (at level 40).
Notation " M ~* Ω " := (resp M cclo Ω) (at level 40).
\end{BVerbatim}
\caption{Syntax for region relations}
\label{regionrel}
\end{figure*}

One important property when proving correctness in Legion is to show that the
relations are equivalent to their closure, with respect to what they imply
about locations in those regions.  Figure~\ref{regionrelproof} for the
statement of this proof. Intuitively, this means that if a region is a
subregion of another region, and that region is a subregion of a third, then
the first subregion is a subregion of the third. Disjointness is commutative, 
and subregions of disjoint regions are disjoint. We've left out the Coq tactic
proofs for space, as they are likely less informative to the reader than the
propositions and definitions, but they can be found online as linked above. 
Lemma \texttt{resp\_clos\_sub} proves that given that if a subregion relation
holds with respect to a mapping \texttt{M} and a set of region constraints
\textOmega{Ω} for all regions \texttt{r1, r2} then holds for the closure of that
set of region constraints under that mapping. Given \texttt{resp\_clos\_sub}, 
Lemma \texttt{resp\_clos} proves that this implies that a mapping respects
a set of region constraints if and onlyo if it respects the closure of that
set. This non-trivial property is crucial for proving that the final decisions 
made by the tasking system based on the sets of constraints preserves 
serial semantics. 

\begin{figure}
\centering
\begin{BVerbatim}
Lemma resp_clos_sub : ∀ r1 r2 Ω (M:M), 
  (∀ r1 r2, subr r1 r2 ∈ Ω → lu r1 M ⊆ lu r2 M) → 
           subr r1 r2 ∈* Ω → lu r1 M ⊆ lu r2 M.
...

Lemma resp_clos : ∀ M Ω, M ~ Ω ↔ M ~* Ω.
...
\end{BVerbatim}
\caption{Lemmas about region relations}
\label{regionrelproof}
\end{figure}

\subsection{Full Implementation of the Specification}
In addition to proving theorems and lemmas about the semantics of Legion as they
relate to HPC memory models, another goal of a formal specification is to ensure
both the completeness and soundness of the Core Legion model. Much of the
necessary implementation work here is ensuring that the entire grammar of the
language is both type safe and its operational semantics contain transition
rules for all productions in the language.

To do this we are writing our proofs of progress and preservation (ie. that well
typed expressions evaluate to a value or diverge and that
every transition in the operational semantics produces a well typed value given
a well typed expression) together with our implementation of the semantics of
the Core Legion language.

Thus far this approach has acted as a sanity check for our implementation and helped us
handle unspecified corner cases in the Core Legion specification (eg. the list
of functions in the \texttt{T-Program} relation has no corresponding transition in the
operational semantics). 

The theorems we are using are described in Figure~\ref{preservationtheorem} and
Figure~\ref{progresstheorem} (see
\texttt{https://github.com/lanl/formallegion} for the in progress proofs). 

\begin{figure}
\begin{BVerbatim}
Theorem preservation : forall (e5:e) (t:T) (E:E) (M:M) (L:L) 
(H:H) (S:list (e * e)) (C:list e) (T:T),
    exists v,
      eval M L H S C e5 v E ->
      typing EmptyContext EmptyPriv EmptyConstraint e5 T  ->
      exists G P Omega T, typing G P Omega v T.
\end{BVerbatim}
\caption{preservation theorem}
\label{preservationtheorem}
\end{figure}

\begin{figure}
\begin{BVerbatim}
Theorem progress : forall  (e5:e) (t:T) (M:M) (H:H)  
(C:list e) (T1 T2:T) L (S:list (e * e)),
    typing EmptyContext EmptyPriv EmptyConstraint e5 T1  ->
    (exists E v ,
        is_value v ->
        eval M L H S C e5 v E).
\end{BVerbatim}
\caption{progress theorem}
\label{progresstheorem}
\end{figure}


\section{Future Work}

The work presented in this paper is a small fraction of a small fraction of the
amount of work required to build a complete, correct-by-construction HPC
application. In this section, we outline some of the possible paths towards
this worthy goal. 

As mentioned earlier in the paper, one of the keystones for large formally
correct systems is modularity. This modularity has two purposes. On the one
hand, it enables ease of implementation. This has long been recognized 
in the field of software engineering \cite{?}. The second, analagous advantage,
is that in a similar way modularity eases the burden of proofs. By defining 
specifications as abstractions, one can separate concerns of proofs. For example,
by composing a proof that a program's serial semantics comply with a formal
specification with a proof that a parallel language respects those serial 
semantics, we can get a proof that a parallel program respects the original
formal semantics. In HPC, by applying this idea to parallel programming models like
Legion, we can separate the application developer from having to prove 
correctness with respect to one or more increasingly complex, heterogeneous
architechures. Instead, we can provide an intermediate layer, like Legion, that
allows application developers to reason and prove correctness to much 
simpler operational semantics, and then prove that those semantics are retained
even on the complex, heterogeneous architectures of todays supercomputers.

Of course this is much easier said than done, and we address some of the challenges
faced by such an effort in the following subsections.

\subsection{Current Limitations}

One common theme in formalizing systems is the discovery of issues in the
corresponding informal specification. For example, when formalizing the C
language for the CompCert project, Leroy et. al. found many cases where the
informal specification was simply incomplete \cite{compcert}. Note that this is
seperate notion from undefined behavior. Like unknown unknowns, gaps in a
specification can make it impossible to prove desired properties of a system.
In contrast, undefined behaviour is only a problem when programs exihibit it.

In formalizing Legion, we discovered that while there is an impressive theory
ensuring that the side effects of concurrent evaluations preserve serial
semantics with respect to memory reads and writes, there is no theory for what
expressions can be evaluated concurrently based on value dependencies. For
example, if we define the program
\begin{center}
\begin{BVerbatim}
Definition read_write x y t T r :=
  elet x t (read (id y)) 
    (write (new T r) (id x)).
\end{BVerbatim}
\end{center}
then the semantics will happily order the \texttt{read} \emph{after} the
\texttt{write}, despite the fact that the value being written, \texttt{x},
generally will need to be read before being written. This isn't catastrophic
for the semantics, as the returned value and resulting heap are equivalent, 
as desired, but it limits the ability to reason effectively about concurrency. 
In the actual implementations this is handled both dynamically with futures
and statically with dependency analysis \cite{regent}.

Another issue with the semantics as they are now is that there is no way to 
constrain ordering on side effects \emph{other than reads and writes into
regions}. For example, generally programs will need to make system calls or
foreign function calls in a particular order, and currently in the semantics
there is no support for enforcing such an ordering.

A third issue is that there is no way to dynamically create and destroy
regions. In other words, it the semantics assumes static memory allocation.
Of course, this differs from the actual implementation, and is an important,
though difficult, semantics to formalize. As memory heirarchies get more 
sophisticated, the need for tools for reasoning about memory use and locality
becomes more essential. 

A fourth challenge is that in practice, Legion is not a stand-alone language.
Instead, it is embedded in an existing language. For the current implementations, 
those languages are Regent and C/C++ \cite{regent, legion}. There are two 
interesting paths towards addressing this issue. One would be to build a sort 
of abstraction for languages that Legion is embedded in, and prove correctness
with respect to that abstraction. This would likely be difficult to get right, as 
capturing the important semantics of the languages that it would be embedded in
would likely be a significant challenge. A second possibility for addressing this 
issue would be choosing a particular semantics and focusing on that. In particular,
combining the C API with the CompCert verified C work would likely be a
fruitful, though challenging, venture. 

All of these issues are addressable given enough time and effort. Some offer
a nice opportunity for modularity within the tasking formalization. For example, 
ordering tasks based on dependencies of values is common accross all tasking
systems \cite{blumofe1995cilk, kaiser2014hpx}, and is likely a sort of sub-semantics that they could
all share, reducing proof burden across different run-times.

\subsection{Performance}

Returning to the "P" in "HPC", one important current area of research is formal
reasoning about performance \cite{performance}. One of the challenges for
Legion, and runtime systems in general, is that the scheduler often has a
non-trivial amount of work to do, leading to large overheads. In Legion, the
combination of reasoning about permissions, memory locality, etc., makes this
overhead particularly hard to predict. 

This is one oft-overlooked advantage to formal reasoning: it allows reasoning
about \emph{any} property of a system, not just correctness. For example, there
is the work by \cite{?} working towards formally verified complexity of code.
Chlipala et. al. have also used techniques like those described here to construct
and verify hardware description, where performance must be reasoned about down
to single cycles \cite{braibant2013formal}.  This is one area where other tasking
systems, like Cilk, have a better story than Legion. Blumofe et al. informally
describe bounds on runtime overheads, which has been realized in practice as
low overheads in the actual runtime \cite{blumofe1995cilk}. 

Ideally, we'd like to make similar claims about Legion runtime overheads.
Given a formal model of the work the runtime system has to do, one can reason
about bounds on overhead. Currently, due to the complexity of the runtime
implementation, this is infeasible. Like some of the desired correctness
properties, a significant simplification of the runtime, and perhaps the
semantics, would likely be required to achieve such a goal.


\subsection{Other Tasking Systems}

While we've focused on Legion, there are other parallel programming systems
that offer exciting opportunities for formalization. While some (looking at
you, OpenMP), are likely much to large and complex to formalize without
herculean effort, others have simple enough semantics that formalization is
likely within reach. One in particular that is attractive due to its simplicity
is the recent work on parallel extensions to LLVM, the Tapir project
\cite{schardl2017tapir}. Building on existing formalization efforts for LLVM
\cite{zhao2012formalizing}, extending existing semantics with the Tapir instructions for
concurrency should be viable. Again, there would be many challenges,
particularly in the area of formal reasoning about memory models, but the
approach should be viable.

As mentioned earlier, there is also the important notion of \emph{shared
semantics accross different tasking systems}. This notion of shared semantics
presents a valuable opportunity for modularization of specification. By defining
a shared semantics developers could reason more effectively, both formally and
informally, about the semantics of their code with respect to tasking layers.

\section{Conclusion}

We hope to have convinced the reader of the importance and viability of proving
parallel programming systems in HPC correct. While we've only taken small steps
on the path to provably correct Legion, our hope is that it will inspire
support for such ventures by providing a convincing story on how to move
forward. There is ever- increasing evidence that this class of deep
specifications and certified software is an incredibly powerful tool for
proving desirable properties about our large software systems. We'd like to see
more focus on correctness in HPC, and we think deep specifications and
certified proofs are the way forward.


\bibliographystyle{ACM-Reference-Format}
\bibliography{annotated}

\end{document}
